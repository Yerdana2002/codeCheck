Detection of vulnerabilities in a code output by Large Language Models (LLMs). 

This project aims to present the usage of Claude 3.7 sonnet by Anthropic as a base model, to detect, and fix vulnerable code outputted by Large Language Models as a part of research done by Siddiq Mohammed, and Joanna Santos. 
Demo, what I learned along the way, description

Apart from aesthetics, there are measurable metrics that are applicable to the project. 
1) Accuracy. I define it as a correct classification of the CWE vulnerability type.
2) Correctness. I define it as correctness of the output for the modified python code.
3) Latency. This is the time period that the end user waits between sending their prompt, and getting their result back. Although the time depends on speed of the network, or complexity of their task in a prompt, it also depends on the design and implementation of the system.
